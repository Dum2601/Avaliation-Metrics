**README ‚Äî C√°lculo de M√©tricas de Avalia√ß√£o de Aprendizado / Evaluation Metrics Calculation Project**

---

### üáßüá∑ Portugu√™s

#### Descri√ß√£o

Este projeto tem como objetivo calcular as principais m√©tricas utilizadas na avalia√ß√£o de modelos de **classifica√ß√£o de dados**. As m√©tricas implementadas s√£o **acur√°cia**, **sensibilidade (recall)**, **especificidade**, **precis√£o** e **F1-score**.

Atrav√©s da implementa√ß√£o manual e tamb√©m com o uso das fun√ß√µes dispon√≠veis na biblioteca `scikit-learn`, o projeto busca demonstrar o funcionamento pr√°tico de cada uma dessas m√©tricas, baseando-se em uma **matriz de confus√£o** previamente definida.

#### F√≥rmulas principais

* **Acur√°cia:** (VP + VN) / (VP + VN + FP + FN)
* **Sensibilidade (Recall):** VP / (VP + FN)
* **Especificidade:** VN / (VN + FP)
* **Precis√£o (Precision):** VP / (VP + FP)
* **F1-Score:** 2 √ó (Precis√£o √ó Recall) / (Precis√£o + Recall)

Onde:

* VP = Verdadeiro Positivo
* VN = Verdadeiro Negativo
* FP = Falso Positivo
* FN = Falso Negativo

#### Exemplo de c√≥digo

```python
from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score

# Dados de exemplo
y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]
y_pred = [1, 0, 1, 0, 0, 1, 0, 1, 1, 0]

print("Acur√°cia:", accuracy_score(y_true, y_pred))
print("Recall:", recall_score(y_true, y_pred))
print("Precis√£o:", precision_score(y_true, y_pred))
print("F1-Score:", f1_score(y_true, y_pred))
print("Matriz de confus√£o:\n", confusion_matrix(y_true, y_pred))
```

#### Objetivo educacional

O prop√≥sito √© compreender o papel de cada m√©trica e sua import√¢ncia na avalia√ß√£o do desempenho de classificadores, principalmente em contextos onde h√° **desequil√≠brio de classes**.

---

### üá¨üáß English 

#### Description

This project aims to calculate the main metrics used for evaluating **data classification models**. The implemented metrics are **accuracy**, **recall (sensitivity)**, **specificity**, **precision**, and **F1-score**.

By implementing both manual formulas and the built-in functions from the `scikit-learn` library, this project demonstrates how each metric operates in practice, based on a predefined **confusion matrix**.

#### Main formulas

* **Accuracy:** (TP + TN) / (TP + TN + FP + FN)
* **Recall (Sensitivity):** TP / (TP + FN)
* **Specificity:** TN / (TN + FP)
* **Precision:** TP / (TP + FP)
* **F1-Score:** 2 √ó (Precision √ó Recall) / (Precision + Recall)

Where:

* TP = True Positive
* TN = True Negative
* FP = False Positive
* FN = False Negative

#### Code example

```python
from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score

# Example data
y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]
y_pred = [1, 0, 1, 0, 0, 1, 0, 1, 1, 0]

print("Accuracy:", accuracy_score(y_true, y_pred))
print("Recall:", recall_score(y_true, y_pred))
print("Precision:", precision_score(y_true, y_pred))
print("F1-Score:", f1_score(y_true, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))
```

#### Educational goal

The purpose is to understand the role and importance of each metric when assessing classifier performance, especially in scenarios with **class imbalance**.
